{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f69ad96",
   "metadata": {},
   "source": [
    "# PyTorch models and Multi-Model Endpoints with the SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee664b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For the MME section, we'll need some additional libraries not available in the notebook kernel by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1673d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker-pytorch-inference>=2\" torch-model-archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28454d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Configuration:\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-rea\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d603c8",
   "metadata": {},
   "source": [
    "You only need to run this cell if you ran the [Custom_Container.ipynb](Custom_Container.ipynb) already and would like to use the built custom container images from there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ff3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r custom_training_uri\n",
    "%store -r custom_inference_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b900b485",
   "metadata": {},
   "source": [
    "## Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_list = sagemaker_session.upload_data(\"./data/list_seq.pickle\", bucket=bucket, key_prefix=prefix+'/train')\n",
    "print(inputs_list)\n",
    "inputs_dict = sagemaker_session.upload_data(\"./data/dict_loc.pickle\", bucket=bucket, key_prefix=prefix+'/train')\n",
    "print(inputs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae7ea3",
   "metadata": {},
   "source": [
    "## Sagemaker Pytorch Estimator - train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69cf231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# place to save model artifact\n",
    "output_path = f\"s3://{bucket}/{prefix}/output/\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    framework_version=\"1.7.1\",\n",
    "    py_version=\"py3\",\n",
    "    # If you built and are using customized containers:\n",
    "    #image_uri=custom_training_uri,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        \"embedding_dims\": 128,\n",
    "        \"initial_lr\": 0.025,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 16,\n",
    "        \"n_workers\": 16,\n",
    "    }\n",
    ")\n",
    "\n",
    "estimator.fit({ \"training\": f\"s3://{bucket}/{prefix}/train\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d97d1",
   "metadata": {},
   "source": [
    "## Deploying to an endpoint\n",
    "\n",
    "The `PyTorchModel` class will **re-pack** the training job result tarball, adding in a `code/` folder of your inference code and re-configuring the container's entrypoint from the previously-specified `train.py` to `inference.py`.\n",
    "\n",
    "**IF** your training job:\n",
    "- Already copied the required code assets to `model_dir/code` (including train.py), and\n",
    "- Used an entry-point `train.py` which will still expose the required functions when imported as a module (e.g. by including `from inference import *` in it)\n",
    "\n",
    "**THEN** this re-packaging is not necessary and a simple `estimator.deploy(...)` will work: Because the container will import `code/train.py` and get the definitions it needs (`model_fn` and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = estimator.latest_training_job.describe()[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_path,\n",
    "    #name=\"pytorch-single-model\",\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    framework_version=\"1.7.1\",\n",
    "    py_version=\"py3\",\n",
    "    # If you built and are using customized containers:\n",
    "    #image_uri=custom_inference_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7fe2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    #endpoint_name=\"pytorch-single-model\",\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    initial_instance_count=1,\n",
    ")\n",
    "\n",
    "# This endpoint expects (and returns) JSON, rather than the default numpy format for the PyTorchPredictor\n",
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c6ec7",
   "metadata": {},
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "Depending on your usage context, you may want to invoke the endpoint via the SageMaker Python SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b786cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict({ \"locationIDInput\": [\"mycty_51549\"], \"count\": 5 })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345367ab",
   "metadata": {},
   "source": [
    "...Or via plain Boto3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82288c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client(\"runtime.sagemaker\")\n",
    "\n",
    "endpoint_name = predictor.endpoint_name\n",
    "single_test = json.dumps({ \"locationIDInput\": [\"mycty_51549\"], \"count\": 5 })\n",
    "\n",
    "print(f\"Invoking endpoint {endpoint_name}...\")\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    ContentType = \"application/json\",\n",
    "    Body = single_test,\n",
    ")\n",
    "result = response[\"Body\"].read().decode(\"utf-8\")\n",
    "print(f\"Predicted label is {result}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700f5c9",
   "metadata": {},
   "source": [
    "## Preparing a model archive for MME with TorchServe\n",
    "\n",
    "PyTorch v1.6+ inference containers use [TorchServe](https://pytorch.org/serve/), for consistency with standard practices on the framework. TorchServe requires a particular [model archive](https://github.com/pytorch/serve/tree/master/model-archiver#torch-model-archiver-for-torchserve) format to load and serve models.\n",
    "\n",
    "In SageMaker, the service itself handles downloading and untaring of SageMaker model zips to endpoint containers: Including loading and unloading for Multi-Model Endpoints.\n",
    "\n",
    "While the PyTorch framework container for a single-model endpoint can build the TorchServe model archive from the input model folder on start-up (because the artifacts have already been downloaded), this is not currently supported on MME - because of the dynamic model loading.\n",
    "\n",
    "Therefore to use a model with a TorchServe-based MME endpoint, we need to:\n",
    "\n",
    "- Convert it to a TorchServe Model Archive ourselves beforehand, and\n",
    "- Use `tar.gz` compression (which SageMaker expects) rather than the standard `.mar` (which is, [just a zip](https://github.com/pytorch/serve/blob/40405f90e3c590638871d92fc6cda48f1dcfe570/model-archiver/model_archiver/model_packaging_utils.py#L198) under the covers anyway)\n",
    "\n",
    "First, let's download and unpack the contents of our deployment-ready `model.tar.gz` to `data/model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a78bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model_s3uri = pytorch_model.repacked_model_data\n",
    "\n",
    "shutil.rmtree(\"data/model\", ignore_errors=True)\n",
    "os.makedirs(\"data/model\", exist_ok=True)\n",
    "\n",
    "!aws s3 cp $input_model_s3uri data/model/model.tar.gz\n",
    "\n",
    "print(f\"Extracting in data/model...\")\n",
    "# Can extract with CLI or Python, whichever is preferred:\n",
    "#!cd data/model && tar -xzvf model.tar.gz\n",
    "with tarfile.open(\"data/model/model.tar.gz\", \"r\") as tar:\n",
    "    tar.extractall(\"data/model\")\n",
    "\n",
    "print(f\"Deleting model.tar.gz...\")\n",
    "os.remove(\"data/model/model.tar.gz\")\n",
    "print(f\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786bfaa5",
   "metadata": {},
   "source": [
    "Now, we can construct a TorchServe model archive.\n",
    "\n",
    "This process will ask us for a **handler service**, but since we're deploying to the PyTorch framework container image we can, [like the container does for single-model endpoints](https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/6610a410c0cf40bcf15267abe722d20d50e77bcf/src/sagemaker_pytorch_serving_container/torchserve.py#L118), reference the default SageMaker PyTorch handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b73b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_pytorch_serving_container import handler_service as default_handler_service\n",
    "\n",
    "default_handler_pyfile = default_handler_service.__file__\n",
    "print(f\"Using SageMaker PyTorch default 'handler_service' from:\\n{default_handler_pyfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model_name = \"MyModel\"\n",
    "os.makedirs(f\"data/torchserve-models/{torch_model_name}\", exist_ok=True)\n",
    "# (-f just forces overwrite if the output archive already exists)\n",
    "# \"tgz\" is a supported archive-format but it creates an archive with a nested folder, which won't work\n",
    "!torch-model-archiver \\\n",
    "    -f \\\n",
    "    --model-name $torch_model_name \\\n",
    "    --handler $default_handler_pyfile \\\n",
    "    --export-path data/torchserve-models \\\n",
    "    --version 1.0 \\\n",
    "    --extra-files data/model/ \\\n",
    "    --archive-format \"no-archive\"\n",
    "\n",
    "# ...So we'll tar it separately after creating un-compressed:\n",
    "tmp_arch_loc = f\"../{torch_model_name}.tar.gz\"\n",
    "!cd data/torchserve-models/$torch_model_name && tar -czvf $tmp_arch_loc ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dee26b",
   "metadata": {},
   "source": [
    "## Deploying a Multi-Model Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca783003",
   "metadata": {},
   "outputs": [],
   "source": [
    "mme_name = \"pytorch-mme\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(f\"Creating MME: {mme_name}\")\n",
    "model_data_prefix = f\"s3://{bucket}/{prefix}/mme-artifacts/{mme_name}/\"\n",
    "print(f\"MME artifact store:\\n{model_data_prefix}\")\n",
    "\n",
    "mme = MultiDataModel(\n",
    "    name=mme_name,\n",
    "    model_data_prefix=model_data_prefix,\n",
    "    # The PyTorchModel is passed just to define container/environment the spec:\n",
    "    model=pytorch_model,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb211a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mme_predictor = mme.deploy(\n",
    "    #endpoint_name=mme_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    ")\n",
    "\n",
    "mme_predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "mme_predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac01e5",
   "metadata": {},
   "source": [
    "At first, there are no models in the MME's S3 prefix and no models available on the MME:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $model_data_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No models visible!\n",
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63717215",
   "metadata": {},
   "source": [
    "## Dynamically adding models to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model_s3uri = mme.add_model(model_data_source=\"./data/torchserve-models/MyModel.tar.gz\", model_data_path=torch_model_name)\n",
    "print(ts_model_s3uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25da75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c33f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Calling model {torch_model_name}...\")\n",
    "mme_predictor.predict(\n",
    "    { \"locationIDInput\": [\"mycty_51549\"], \"count\": 5 },\n",
    "    target_model=torch_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName = mme_name,\n",
    "    ContentType = \"application/json\",\n",
    "    TargetModel = torch_model_name,\n",
    "    Body = single_test,\n",
    ")\n",
    "\n",
    "result = response[\"Body\"].read().decode(\"utf-8\")\n",
    "print(f\"Predicted label is {result}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a8d29",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "mme_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mme.delete_model()\n",
    "pytorch_model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ba1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
